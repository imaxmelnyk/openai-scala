package dev.maxmelnyk.openaiscala.models.settings

import dev.maxmelnyk.openaiscala.models.Models

/**
 * [[https://platform.openai.com/docs/api-reference/chat/create]]
 *
 * @param model ID of the model to use.
 * @param temperature What sampling temperature to use, between 0 and 2.
 *                    Higher values like 0.8 will make the output more random,
 *                    while lower values like 0.2 will make it more focused and deterministic.
 *                    We generally recommend altering this or [[topP]] but not both.
 * @param topP An alternative to sampling with temperature, called nucleus sampling,
 *             where the model considers the results of the tokens with [[topP]] probability mass.
 *             So 0.1 means only the tokens comprising the top 10% probability mass are considered.
 *             We generally recommend altering this or [[temperature]] but not both.
 * @param n How many chat completion choices to generate for each input message.
 * @param stop Up to 4 sequences where the API will stop generating further tokens.
 * @param maxTokens The maximum number of tokens allowed for the generated answer.
 *                  By default, the number of tokens the model can return will be (4096 - prompt tokens).
 * @param presencePenalty Number between -2.0 and 2.0.
 *                        Positive values penalize new tokens based on whether they appear in the text so far,
 *                        increasing the model's likelihood to talk about new topics.
 * @param frequencyPenalty Number between -2.0 and 2.0.
 *                         Positive values penalize new tokens based on their existing frequency in the text so far,
 *                         decreasing the model's likelihood to repeat the same line verbatim.
 * @param logitBias Modify the likelihood of specified tokens appearing in the completion.
 *                  Accepts a `Map` that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100.
 *                  Mathematically, the bias is added to the logits generated by the model prior to sampling.
 *                  The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
 *                  values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
 * @param user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
 */
case class CreateChatCompletionSettings(model: String = Models.gpt35Turbo,
                                        temperature: Option[Double] = None,
                                        topP: Option[Double] = None,
                                        n: Option[Long] = None,
                                        stop: Option[Seq[String]] = None,
                                        maxTokens: Option[Long] = None,
                                        presencePenalty: Option[Double] = None,
                                        frequencyPenalty: Option[Double] = None,
                                        logitBias: Option[Map[String, Long]] = None,
                                        user: Option[String] = None)
