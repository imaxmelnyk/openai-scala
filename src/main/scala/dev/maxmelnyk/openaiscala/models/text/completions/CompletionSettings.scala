package dev.maxmelnyk.openaiscala.models.text.completions

import dev.maxmelnyk.openaiscala.models.models.Models

/**
 * [[https://platform.openai.com/docs/api-reference/completions]]
 *
 * @param model ID of the model to use.
 * @param suffix The suffix that comes after a completion of inserted text.
 * @param maxTokens The maximum number of tokens to generate in the completion.
 *                  The token count of your prompt plus [[maxTokens]] cannot exceed the model's context length.
 *                  Most models have a context length of 2048 tokens (except for the newest models, which support 4096).
 * @param temperature What sampling temperature to use, between 0 and 2.
 *                    Higher values like 0.8 will make the output more random,
 *                    while lower values like 0.2 will make it more focused and deterministic.
 *                    We generally recommend altering this or [[topP]] but not both.
 * @param topP An alternative to sampling with temperature, called nucleus sampling,
 *             where the model considers the results of the tokens with [[topP]] probability mass.
 *             So 0.1 means only the tokens comprising the top 10% probability mass are considered.
 *             We generally recommend altering this or [[temperature]] but not both.
 * @param n How many completions to generate for each prompt.
 *          Note: Because this parameter generates many completions, it can quickly consume your token quota.
 *                Use carefully and ensure that you have reasonable settings for [[maxTokens]] and [[stop]].
 * @param logprobs Include the log probabilities on the [[logprobs]] most likely tokens, as well the chosen tokens.
 *                 For example, if [[logprobs]] is 5, the API will return a list of the 5 most likely tokens.
 *                 The API will always return the logprob of the sampled token,
 *                 so there may be up to [[logprobs]]+1 elements in the response.
 *                 The maximum value for logprobs is 5. If you need more than this,
 *                 please contact us through the help center and describe your use case.
 * @param echo Echo back the prompt in addition to the completion.
 * @param stop Up to 4 sequences where the API will stop generating further tokens.
 *             The returned text will not contain the stop sequence.
 * @param presencePenalty Number between -2.0 and 2.0.
 *                        Positive values penalize new tokens based on whether they appear in the text so far,
 *                        increasing the model's likelihood to talk about new topics.
 * @param frequencyPenalty Number between -2.0 and 2.0.
 *                         Positive values penalize new tokens based on their existing frequency in the text so far,
 *                         decreasing the model's likelihood to repeat the same line verbatim.
 * @param bestOf Generates [[bestOf]] completions server-side and returns the "best" (the one with the highest log probability per token).
 *               When used with [[n]], [[bestOf]] controls the number of candidate completions and [[n]] specifies how many to return â€“ [[bestOf]] must be greater than [[n]].
 *               Note: Because this parameter generates many completions, it can quickly consume your token quota.
 *                     Use carefully and ensure that you have reasonable settings for [[maxTokens]] and [[stop]].
 * @param logitBias Modify the likelihood of specified tokens appearing in the completion.
 *                  Accepts a `Map` that maps tokens (specified by their token ID in the GPT tokenizer) to an associated bias value from -100 to 100.
 *                  You can use the tokenizer tool (which works for both GPT-2 and GPT-3) to convert text to token IDs.
 *                  Mathematically, the bias is added to the logits generated by the model prior to sampling.
 *                  The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
 *                  values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
 *                  As an example, you can pass `Some(Map("50256" -> -100))` to prevent the `<|endoftext|>` token from being generated.
 * @param user A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
 */
case class CompletionSettings(model: String = Models.textDavinci003,
                              suffix: Option[String] = None,
                              maxTokens: Option[Long] = None,
                              temperature: Option[Double] = None,
                              topP: Option[Double] = None,
                              n: Option[Long] = None,
                              logprobs: Option[Long] = None,
                              echo: Option[Boolean] = None,
                              stop: Option[Seq[String]] = None,
                              presencePenalty: Option[Double] = None,
                              frequencyPenalty: Option[Double] = None,
                              bestOf: Option[Long] = None,
                              logitBias: Option[Map[String, Long]] = None,
                              user: Option[String] = None)
